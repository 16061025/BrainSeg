{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmvBb5bhWFxx"
   },
   "source": [
    "Brain Tumor Segmentation with U-net model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ2k2AVrV_n2"
   },
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Worj5xTgVdB1"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from skimage.color import rgb2gray\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.layers import Input, Activation, BatchNormalization, Dropout, Lambda, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6luss0yy6wk1"
   },
   "source": [
    "Load mask and brain image into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUQBlkXLrYAs"
   },
   "outputs": [],
   "source": [
    "img_width = 256\n",
    "img_height = 256\n",
    "data_path = \"/scratch/pl2313/dataset/kaggle_3m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXBX6_btV_R6",
    "outputId": "93bc1881-0871-4202-b3d0-9f0a611aa4c7"
   },
   "outputs": [],
   "source": [
    "img = []\n",
    "masks = glob(data_path+'/*/*_mask*')\n",
    "for file in masks:\n",
    "  img.append(file.replace('_mask',''))  \n",
    "\n",
    "print(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCQKDu__AWhL"
   },
   "source": [
    "Separate each dataset into training set, test set and validation set, store them into Dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqktNy0TAcrS"
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data={'brain': img, 'mask': masks})\n",
    "train_dataset, test_dataset = train_test_split(data, test_size = 0.1)\n",
    "train_dataset, validation_dataset = train_test_split(train_dataset,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQTnRmlYqA6g"
   },
   "source": [
    "Data normalize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3IzrXICp-Qt"
   },
   "outputs": [],
   "source": [
    "def normalize_data(img,mask):\n",
    "    img = img / 255\n",
    "    mask = mask / 255\n",
    "    mask[mask > 0.5] = 1\n",
    "    mask[mask <= 0.5] = 0\n",
    "    \n",
    "    return (img, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPXH7PfvpOd1"
   },
   "source": [
    "Training Data generator. Generate image and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbKnAxONpahI"
   },
   "outputs": [],
   "source": [
    "def data_generator(data_frame, batch_size, aug_dict,image_color_mode=\"rgb\",\n",
    "                   mask_color_mode=\"grayscale\", image_save_prefix=\"image\",\n",
    "                  mask_save_prefix=\"mask\",save_to_dir=None, target_size=(256,256), seed=1):\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_dataframe(\n",
    "        data_frame,\n",
    "        x_col = \"brain\",\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_dataframe(\n",
    "        data_frame,\n",
    "        x_col = \"mask\",\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "\n",
    "    train_gen = zip(image_generator, mask_generator)\n",
    "    for (img, mask) in train_gen:\n",
    "        img, mask = normalize_data(img, mask)\n",
    "        yield (img,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eorUZgAqM5H"
   },
   "source": [
    "Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8OBVYTMqO6T"
   },
   "outputs": [],
   "source": [
    "SMOOTH = 10\n",
    "\n",
    "def dice_coef(target, pred):\n",
    "    y_targetf=K.flatten(target)\n",
    "    y_predf=K.flatten(pred)\n",
    "    And=K.sum(y_targetf* y_predf)\n",
    "    return((2* And + SMOOTH) / (K.sum(y_targetf) + K.sum(y_predf) + SMOOTH))\n",
    "\n",
    "def dice_coef_loss(target, pred):\n",
    "    return -dice_coef(target, pred)\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    sum_ = K.sum(y_true + y_pred)\n",
    "    jac = (intersection + SMOOTH) / (sum_ - intersection + SMOOTH)\n",
    "    return jac\n",
    "\n",
    "def jac_distance(y_true, y_pred):\n",
    "    y_truef=K.flatten(y_true)\n",
    "    y_predf=K.flatten(y_pred)\n",
    "\n",
    "    return - iou(y_true, y_pred)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BlGgZzmrAOR"
   },
   "source": [
    "Unet model using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zexzh8ohrBNB"
   },
   "outputs": [],
   "source": [
    "def unet(input_size=(img_width, img_height, 3)):\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    conv1 = Conv2D(64, (3, 3), padding='same')(inputs)\n",
    "    bn1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(64, (3, 3), padding='same')(bn1)\n",
    "    bn1 = BatchNormalization(axis=3)(conv1)\n",
    "    bn1 = Activation('relu')(bn1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)\n",
    "\n",
    "    conv2 = Conv2D(128, (3, 3), padding='same')(pool1)\n",
    "    bn2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(128, (3, 3), padding='same')(bn2)\n",
    "    bn2 = BatchNormalization(axis=3)(conv2)\n",
    "    bn2 = Activation('relu')(bn2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)\n",
    "\n",
    "    conv3 = Conv2D(256, (3, 3), padding='same')(pool2)\n",
    "    bn3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(256, (3, 3), padding='same')(bn3)\n",
    "    bn3 = BatchNormalization(axis=3)(conv3)\n",
    "    bn3 = Activation('relu')(bn3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)\n",
    "\n",
    "    conv4 = Conv2D(512, (3, 3), padding='same')(pool3)\n",
    "    bn4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(512, (3, 3), padding='same')(bn4)\n",
    "    bn4 = BatchNormalization(axis=3)(conv4)\n",
    "    bn4 = Activation('relu')(bn4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(bn4)\n",
    "\n",
    "    conv5 = Conv2D(1024, (3, 3), padding='same')(pool4)\n",
    "    bn5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(1024, (3, 3), padding='same')(bn5)\n",
    "    bn5 = BatchNormalization(axis=3)(conv5)\n",
    "    bn5 = Activation('relu')(bn5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(bn5), conv4], axis=3)\n",
    "    conv6 = Conv2D(512, (3, 3), padding='same')(up6)\n",
    "    bn6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(512, (3, 3), padding='same')(bn6)\n",
    "    bn6 = BatchNormalization(axis=3)(conv6)\n",
    "    bn6 = Activation('relu')(bn6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(bn6), conv3], axis=3)\n",
    "    conv7 = Conv2D(256, (3, 3), padding='same')(up7)\n",
    "    bn7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(256, (3, 3), padding='same')(bn7)\n",
    "    bn7 = BatchNormalization(axis=3)(conv7)\n",
    "    bn7 = Activation('relu')(bn7)\n",
    "\n",
    "    up8 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(bn7), conv2], axis=3)\n",
    "    conv8 = Conv2D(128, (3, 3), padding='same')(up8)\n",
    "    bn8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(128, (3, 3), padding='same')(bn8)\n",
    "    bn8 = BatchNormalization(axis=3)(conv8)\n",
    "    bn8 = Activation('relu')(bn8)\n",
    "\n",
    "    up9 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(bn8), conv1], axis=3)\n",
    "    conv9 = Conv2D(64, (3, 3), padding='same')(up9)\n",
    "    bn9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(64, (3, 3), padding='same')(bn9)\n",
    "    bn9 = BatchNormalization(axis=3)(conv9)\n",
    "    bn9 = Activation('relu')(bn9)\n",
    "\n",
    "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(bn9)\n",
    "\n",
    "    return Model(inputs=[inputs], outputs=[conv10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJH1_fthrrm5",
    "outputId": "1c67966b-b1b4-4a9c-d748-af70995d2291"
   },
   "outputs": [],
   "source": [
    "unet_model = unet()\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAKeKJCSsKMN",
    "outputId": "c13d5dbb-6939-483a-c767-e737670b4699"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "decay = learning_rate / EPOCHS\n",
    "# optimizer\n",
    "optimizer = Adam(lr=learning_rate, decay=decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTiK9LLOtYOe"
   },
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6x9bN_dsP_Q"
   },
   "outputs": [],
   "source": [
    "train_generator_args = dict(rotation_range=0.2,\n",
    "                            width_shift_range=0.05,\n",
    "                            height_shift_range=0.05,\n",
    "                            shear_range=0.05,\n",
    "                            zoom_range=0.05,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "train_data = data_generator(train_dataset, BATCH_SIZE, train_generator_args, target_size=(img_height, img_width))\n",
    "\n",
    "val_data =  data_generator(validation_dataset, BATCH_SIZE,dict(),target_size=(img_height, img_width))\n",
    "\n",
    "callbacks = [ModelCheckpoint('unet_brain_mri_seg.hdf5', verbose=1, save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "818RLcmXsK1K"
   },
   "source": [
    "Train the model using dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wfhZIP4ktbNL",
    "outputId": "943ea794-c21b-4fc5-f292-144e855c1657"
   },
   "outputs": [],
   "source": [
    "unet_model.compile(optimizer=optimizer, loss=dice_coef_loss, metrics=[\"binary_accuracy\", iou, dice_coef])\n",
    "result = unet_model.fit(train_data,\n",
    "                    steps_per_epoch=len(train_dataset) / BATCH_SIZE, \n",
    "                    epochs=EPOCHS, \n",
    "                    callbacks=callbacks,\n",
    "                    validation_data = val_data,\n",
    "                    validation_steps=len(validation_dataset) / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJhEDNuPKzhS"
   },
   "outputs": [],
   "source": [
    "a = result.history\n",
    "\n",
    "list_traindice = a['dice_coef']\n",
    "list_testdice = a['val_dice_coef']\n",
    "\n",
    "list_trainjaccard = a['iou']\n",
    "list_testjaccard = a['val_iou']\n",
    "\n",
    "list_trainloss = a['loss']\n",
    "list_testloss = a['val_loss']\n",
    "plt.figure(1)\n",
    "plt.plot(list_testloss, 'b-')\n",
    "plt.plot(list_trainloss,'r-')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('loss graph', fontsize = 15)\n",
    "plt.figure(2)\n",
    "plt.plot(list_traindice, 'r-')\n",
    "plt.plot(list_testdice, 'b-')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('accuracy graph', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vLqQaDHLBAY"
   },
   "outputs": [],
   "source": [
    "model = load_model('unet_brain_mri_seg.hdf5', custom_objects={'dice_coef_loss': dice_coef_loss, 'iou': iou, 'dice_coef': dice_coef})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dexkNMOLLEvy"
   },
   "outputs": [],
   "source": [
    "test_gen = data_generator(test_dataset, BATCH_SIZE,\n",
    "                                dict(),\n",
    "                                target_size=(img_height, img_width))\n",
    "results = model.evaluate(test_gen, steps=len(test_dataset) / BATCH_SIZE)\n",
    "print(\"Test lost: \",results[0])\n",
    "print(\"Test IOU: \",results[1])\n",
    "print(\"Test Dice Coefficent: \",results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMjk3cPULHcl"
   },
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    index=np.random.randint(1,len(test_dataset.index))\n",
    "    img = cv2.imread(test_dataset['filename'].iloc[index])\n",
    "    img = cv2.resize(img ,(img_height, img_width))\n",
    "    img = img / 255\n",
    "    img = img[np.newaxis, :, :, :]\n",
    "    pred=model.predict(img)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(np.squeeze(img))\n",
    "    plt.title('Original Image')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(np.squeeze(cv2.imread(test_dataset['mask'].iloc[index])))\n",
    "    plt.title('Original Mask')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(np.squeeze(pred) > .5)\n",
    "    plt.title('Prediction')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "brain_tumor_segmentation.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
